{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO4/FrR3OipWZDBtv4mh3+1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# AI-Powered Anomaly Detection PoC for Subledger Transactions\n",
        "\n",
        "##**Purpose of this Notebook**\n",
        "\n",
        "This notebook demonstrates an end-to-end Proof of Concept (PoC) for applying Machine Learning (ML) and Large Language Models (LLMs) to detect and explain anomalies in a subledger accounting dataset.\n",
        "\n",
        "The workflow showcases how an accounting or finance platform can:\n",
        "\n",
        "1. Ingest transaction data\n",
        "2. Engineer meaningful features (dates, accounts, user behaviour, amounts)\n",
        "3. Apply an IsolationForest model to detect unusual transactions\n",
        "4. Compute statistical baselines (per account, per vendor, etc.) to give context about “normal” behaviour\n",
        "5. Use OpenAI models to generate clear, domain-specific explanations for each anomaly\n",
        "6. Produce enriched output suitable for dashboards, audit workflows, or finance review queues\n",
        "\n",
        "---\n",
        "\n",
        "##**What this PoC demonstrates**\n",
        "\n",
        "- A scalable approach for automated anomaly detection\n",
        "- How ML models and LLMs can work together:\n",
        "- ML flags anomalies\n",
        "- LLM explains why, using statistical context\n",
        "- A simple, portable pipeline that an engineering team can later convert into production services\n",
        "- How to ground LLM reasoning in real data to avoid hallucinations\n",
        "\n",
        "---\n",
        "\n",
        "##**Who this is for**\n",
        "\n",
        "- The Product and Engineering team evaluating AI use cases and costs\n",
        "- Engineering teams exploring how ML + LLMs can enhance auditability\n",
        "- Finance SMEs who want to understand how AI can surface insights in everyday workflows\n",
        "- Stakeholders who want a clear, transparent demonstration—not a black box\n",
        "\n",
        "---\n",
        "\n",
        "##**What this is NOT**\n",
        "\n",
        "- Not a fully tuned production model\n",
        "- Not a fully secure, optimized, or scalable architecture\n",
        "- Not a replacement for human review\n",
        "- Not tied to any specific client implementation as it uses fabricated data\n",
        "\n",
        "This is a transparent, educational PoC that shows how AI can be integrated into real workflows.\n",
        "\n",
        "---\n",
        "\n",
        "##**Next Steps**\n",
        "\n",
        "- Fine tune and understand IF better\n",
        "- Add in a rule layer, potentially flagging things that IF wouldn't. For example large transactions might not be flagged if in keeping with other regular patterns. This might mean IF doesn't pick it up but it should be flagged.\n",
        "\n",
        "---\n",
        "\n",
        "##**High-Level Architecture Overview**\n",
        "\n",
        "At a high level, this PoC follows this flow:\n",
        "\n",
        "- Synthetic or real subledger data\n",
        "  - Input: transaction records (date, amount, accounts, user, vendor, type…)\n",
        "  - Source: CSV file (transactions.csv)\n",
        "- Feature engineering\n",
        "  - Derive extra fields: hour of day, day of week, month, log-transformed amount.\n",
        "  - Build a feature matrix suitable for ML models.\n",
        "- Statistical profiling\n",
        "  - Compute per-account and per-vendor statistics:\n",
        "    - Average / median amounts\n",
        "    - Transaction counts\n",
        "    - Z-scores vs typical amounts\n",
        "  - These provide ground truth “typical behaviour” for later explanations.\n",
        "- Anomaly detection (IsolationForest)\n",
        "  - Train an IsolationForest model on the engineered features.\n",
        "  - Score each transaction for how unusual it is.\n",
        "  - Select the top N most anomalous transactions.\n",
        "- LLM-powered explanations (OpenAI)\n",
        "  - For each anomalous transaction, send:\n",
        "    - The raw transaction fields\n",
        "    - Anomaly score\n",
        "    - Statistical context (per-account/per-vendor stats)\n",
        "  - The LLM returns:\n",
        "    - Severity (low / medium / high)\n",
        "    - Category (e.g. amount_anomaly, timing_anomaly…)\n",
        "    - Human-readable explanation\n",
        "    - Suggested next action for a finance user\n",
        "- Enriched anomaly output\n",
        "  - Combine ML output + LLM output into a single DataFrame.\n",
        "  - Export as CSV for:\n",
        "    - Internal review\n",
        "    - Product design discussions\n",
        "    - Stakeholder demos\n",
        "\n",
        "Visually, you can think of it as:\n",
        "\n",
        "```\n",
        "CSV → Features & Stats → IsolationForest → Top Anomalies → OpenAI Explanations → Enriched Output\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "##**Why IsolationForest for This PoC**\n",
        "\n",
        "IsolationForest is a strong match for anomaly detection in financial and accounting data, especially at the PoC stage. It was selected for several reasons:\n",
        "\n",
        "1. Purpose-built for anomaly detection\n",
        "\n",
        "IsolationForest is specifically designed to detect rare, unusual, or outlier behaviours, which is exactly what we want when reviewing subledger transactions.\n",
        "\n",
        "It works by:\n",
        "\n",
        "- Randomly “splitting” the data many times\n",
        "- Checking how easily a data point can be isolated\n",
        "- Outliers are isolated quickly → scored as anomalous\n",
        "\n",
        "This works well for transactions whose amounts, timing, or account combinations differ materially from typical patterns.\n",
        "\n",
        "2. No labels required (unsupervised learning)\n",
        "\n",
        "    In accounting systems, true anomalies are:\n",
        "\n",
        "    - Rare\n",
        "    - Expensive to label\n",
        "    - Often subjective\n",
        "\n",
        "    IsolationForest requires no ground-truth labels, making it ideal for:\n",
        "\n",
        "    - POCs\n",
        "    - Legacy datasets without classifications\n",
        "    - Early stages of an anomaly detection project\n",
        "\n",
        "3. Handles high-dimensional + mixed data\n",
        "\n",
        "   Subledger transactions have a mix of:\n",
        "    - Numeric fields (amount, hour, month)\n",
        "    - Categorical fields (account, vendor, user, type)\n",
        "\n",
        "    IsolationForest works well when combined with:\n",
        "\n",
        "    - One-hot encoding for categorical variables\n",
        "    - Numeric transformations (log amounts)\n",
        "\n",
        "This gives it the flexibility needed for financial data structures.\n",
        "\n",
        "4. Fast, lightweight, and scalable\n",
        "\n",
        "IsolationForest:\n",
        "\n",
        "- Trains quickly (even on large datasets)\n",
        "- Is memory-efficient\n",
        "- Offers predictable inference performance\n",
        "\n",
        "This makes it suitable as a backend microservice in a SaaS product.\n",
        "\n",
        "5. Works well with LLM explanations\n",
        "\n",
        "IsolationForest provides:\n",
        "\n",
        "- An anomaly score\n",
        "- A binary anomaly flag\n",
        "\n",
        "But it doesn’t explain why a transaction is unusual.\n",
        "This makes it a perfect pairing with LLMs:\n",
        "\n",
        "- IsolationForest identifies suspicious transactions\n",
        "- The LLM interprets and explains them using statistical context\n",
        "\n",
        "This combination provides:\n",
        "\n",
        "- Transparency\n",
        "- Auditability\n",
        "- User trust\n",
        "- High-value insights for finance teams\n",
        "\n",
        "6. A simple, transparent model for stakeholders\n",
        "\n",
        "Since this is a PoC, we wanted a model that:\n",
        "- Stakeholders can understand\n",
        "- Isn’t “too magical”\n",
        "- Requires minimal ML expertise to explain\n",
        "\n",
        "IsolationForest fits that perfectly.\n",
        "\n",
        "Here is a video explaining IsolationForest in more detail and visuals\n",
        "\n",
        "[IsolationForest](https://www.youtube.com/watch?v=kN--TRv1UDY\n",
        "\n",
        "---\n",
        "\n",
        "##**How to Run This Notebook**\n",
        "\n",
        "To run this PoC end-to-end:\n",
        "\n",
        "If the notebook has been shared via Google,\n",
        "\n",
        "1. Open in Google Colab (if the link has not been shared with you)\n",
        "   - Upload the notebook to Colab or open it from GitHub/Drive.\n",
        "\n",
        "2. Set your OpenAI API key\n",
        "   - In Colab, go to: Left sidebar → Key Icon\n",
        "   - Add a new variable:\n",
        "     - Enable Notebook Access\n",
        "     - Name: OPENAI_API_KEY\n",
        "     - Value: your OpenAI secret key (sk-...)\n",
        "\n",
        "3. Run all cells in order\n",
        "   - Use: Runtime → Run all\n",
        "   - Or step through each cell from top to bottom.\n",
        "   - The notebook will:\n",
        "     - Generate or load transaction data\n",
        "     - Build features and stats\n",
        "     - Train IsolationForest\n",
        "     - Call OpenAI to explain anomalies\n",
        "     - Produce an enriched anomalies table\n",
        "   - Inspect the results\n",
        "     - Look at the df_enriched.head() outputs.\n",
        "     - Check the CSV file (e.g. enriched_anomalies_openai.csv) in the Colab file browser. (it'll be in the `Content` folder)\n",
        "     - Optionally download it for further analysis or inclusion in slides.\n",
        "   - Modify configuration as needed\n",
        "     - Adjust TOP_N (number of anomalies to explain).\n",
        "     - Tweak the IsolationForest contamination rate.\n",
        "     - Edit the system prompt to refine explanation style or categories.\n",
        "\n",
        "\n",
        "I'll stop waffling now and you can get to testing!!!"
      ],
      "metadata": {
        "id": "RUMzqHQsvQes"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1 — Install Required Python Packages**\n",
        "\n",
        "This step ensures that all necessary Python libraries are available in the Colab environment.\n",
        "\n",
        "- Installs pandas - Used for loading, cleaning, merging, and analysing transaction data.\n",
        "\n",
        "- Installs scikit-learn - Provides the IsolationForest anomaly detection model and preprocessing utilities (OneHotEncoder, Pipelines, etc.).\n",
        "\n",
        "- Updates the openai client SDK to the latest version. Required for calling OpenAI’s LLMs to generate explanations for anomalies. This ensures we have the latest features such as structured output (response_format={\"type\":\"json_object\"}).\n",
        "\n",
        "Because Google Colab resets packages each session, this cell is required every time you open the notebook."
      ],
      "metadata": {
        "id": "aSPQIGN8vixj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "aHLBh5Ofruwm",
        "outputId": "c8ecd5ad-54ef-4b0e-bde3-d95d963b557e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (2.8.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.12.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.11.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas scikit-learn\n",
        "!pip install --upgrade openai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2 — Import Required Libraries**\n",
        "\n",
        "In this step we import the core libraries used throughout the notebook.\n",
        "\n",
        "What they’re for:\n",
        "\n",
        "- pandas, numpy – work with tabular data and numbers.\n",
        "- IsolationForest – the anomaly detection model.\n",
        "- ColumnTransformer, OneHotEncoder, Pipeline – build a preprocessing + modelling pipeline that:\n",
        "  - keeps numeric features as-is\n",
        "  - one-hot encodes categorical features\n",
        "  - runs everything through IsolationForest.\n",
        "- json – formats transactions to send to the LLM and parses JSON responses."
      ],
      "metadata": {
        "id": "RWBfFPi0vrRw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "import json"
      ],
      "metadata": {
        "id": "ltFM9kXev25-"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3 — Configure OpenAI Client**\n",
        "\n",
        "This step sets up access to the OpenAI API so we can generate explanations for anomalous transactions.\n",
        "\n",
        "What’s happening:\n",
        "\n",
        "- Load API key from Colab’s secure storage\n",
        "The key should be added in Colab → Settings → Variables under OPENAI_API_KEY.\n",
        "- Initialize the OpenAI client\n",
        "This lets the notebook send requests to the OpenAI models.\n",
        "- Select the model\n",
        "gpt-4.1-mini is fast, inexpensive, and strong enough for structured anomaly explanations."
      ],
      "metadata": {
        "id": "ZPU99ZRUv8KL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from openai import OpenAI\n",
        "\n",
        "api_key = userdata.get(\"OPENAI_API_KEY\")\n",
        "client = OpenAI(api_key=api_key)\n",
        "\n",
        "MODEL_NAME = \"gpt-4.1-mini\"  # good default for this use case"
      ],
      "metadata": {
        "id": "LK1_r1vpwDky"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4 — Generate Synthetic Transaction Data with Built-In Anomalies**\n",
        "\n",
        "This step creates a fake subledger dataset so the PoC can run even without real customer data. The data is shaped to look realistic and includes some deliberately unusual transactions for the model to find.\n",
        "\n",
        "**What the code does:**\n",
        "\n",
        "1. Sets up randomness and dataset size\n",
        "   - rng = np.random.default_rng(42) makes results reproducible.\n",
        "   - n_rows = 5000 controls how many transactions we simulate.\n",
        "2. Builds realistic base data\n",
        "   - Date range over the last 180 days, mostly in business hours (08:00–20:00).\n",
        "   - Random currencies, accounts, users, vendors, and transaction types.\n",
        "   - Transaction amounts drawn from a log-normal distribution (skewed like real financial data), with mostly positive and some negative amounts.\n",
        "3. Creates a DataFrame df\n",
        "   - Columns: transaction_id, date_time, amount, currency, debit_account, credit_account, user_id, vendor, transaction_type.\n",
        "4. Injects clear anomalies\n",
        "   - Very large amounts for some rows (50k–250k).\n",
        "   - Weird account combinations using 9999-Miscellaneous / 8888-Suspense.\n",
        "   - Night-time postings between 00:00–04:00.\n",
        "   - Marks these rows with is_synthetic_anomaly = True so we can later check if the model finds them.\n",
        "5. Quick sanity check\n",
        "   - df.head() shows a sample of rows.\n",
        "   - df[\"is_synthetic_anomaly\"].value_counts() shows how many anomalies were injected.\n",
        "\n",
        "**Things you can tweak:**\n",
        "\n",
        "- n_rows – scale up/down the size of the dataset.\n",
        "- The account lists, vendors, and currencies – to better match our use cases domain.\n",
        "- n_anomalies - scale up/down the number of anomalies you want to inject.\n",
        "- The anomaly patterns:\n",
        "  - Range of “very large” amounts.\n",
        "  - How many anomalies: n_anomalies.\n",
        "  - Definitions of “weird” accounts or times.\n",
        "- The probabilities for transaction_types to change the mix of invoices/payments/journals/adjustments.\n",
        "\n",
        "These knobs let you simulate different risk profiles or transaction behaviours for experimentation."
      ],
      "metadata": {
        "id": "BRAFQn6byRbE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rng = np.random.default_rng(42)\n",
        "\n",
        "n_rows = 5000\n",
        "\n",
        "# 1) Date range: last 180 days\n",
        "date_range = pd.date_range(end=pd.Timestamp.today(), periods=180, freq=\"D\")\n",
        "dates = rng.choice(date_range, size=n_rows)\n",
        "\n",
        "# 2) Currencies, accounts, users, vendors, transaction types\n",
        "currencies = [\"USD\", \"EUR\", \"GBP\"]\n",
        "debit_accounts = [\n",
        "    \"6000-Marketing\",\n",
        "    \"6100-Travel\",\n",
        "    \"6200-IT\",\n",
        "    \"6300-ProfessionalFees\",\n",
        "    \"6400-Rent\",\n",
        "    \"6500-Salaries\",\n",
        "]\n",
        "credit_accounts = [\n",
        "    \"1000-Cash\",\n",
        "    \"2000-AP\",\n",
        "    \"2100-AccruedExpenses\",\n",
        "    \"2200-DeferredRevenue\",\n",
        "]\n",
        "users = [f\"user_{i}\" for i in range(1, 16)]\n",
        "vendors = [\n",
        "    \"ACME Events\",\n",
        "    \"Globex IT\",\n",
        "    \"Initech Consulting\",\n",
        "    \"Stark Supplies\",\n",
        "    \"Wayne Travel\",\n",
        "    \"Umbrella Services\",\n",
        "]\n",
        "transaction_types = [\"invoice\", \"payment\", \"journal\", \"adjustment\"]\n",
        "\n",
        "# 3) Amounts: mostly \"normal\" with some randomness\n",
        "# Use log-normal-like distribution for realistic skew\n",
        "base_amounts = rng.lognormal(mean=7, sigma=0.5, size=n_rows)  # around a few thousand\n",
        "signs = rng.choice([1, -1], size=n_rows, p=[0.8, 0.2])  # mostly positive, some negative\n",
        "amounts = np.round(base_amounts * signs, 2)\n",
        "\n",
        "# 4) Assemble base dataframe\n",
        "df = pd.DataFrame({\n",
        "    \"transaction_id\": [f\"tx_{i}\" for i in range(1, n_rows + 1)],\n",
        "    \"date_time\": dates + pd.to_timedelta(rng.integers(8, 20, size=n_rows), unit=\"h\"),  # business hours 08:00–20:00\n",
        "    \"amount\": amounts,\n",
        "    \"currency\": rng.choice(currencies, size=n_rows),\n",
        "    \"debit_account\": rng.choice(debit_accounts, size=n_rows),\n",
        "    \"credit_account\": rng.choice(credit_accounts, size=n_rows),\n",
        "    \"user_id\": rng.choice(users, size=n_rows),\n",
        "    \"vendor\": rng.choice(vendors, size=n_rows),\n",
        "    \"transaction_type\": rng.choice(transaction_types, size=n_rows, p=[0.4, 0.3, 0.2, 0.1]),\n",
        "})\n",
        "\n",
        "# 5) Inject some \"obvious\" anomalies so we can see if the model finds them\n",
        "n_anomalies = 40\n",
        "anomaly_indices = rng.choice(df.index, size=n_anomalies, replace=False)\n",
        "\n",
        "# a) Very large amounts\n",
        "large_idx = anomaly_indices[:15]\n",
        "df.loc[large_idx, \"amount\"] = np.round(rng.uniform(50000, 250000, size=len(large_idx)), 2)\n",
        "\n",
        "# b) Weird account combinations (e.g., rent against cash, salaries against deferred revenue)\n",
        "weird_idx = anomaly_indices[15:30]\n",
        "df.loc[weird_idx, \"debit_account\"] = rng.choice(\n",
        "    [\"9999-Miscellaneous\", \"8888-Suspense\"], size=len(weird_idx)\n",
        ")\n",
        "df.loc[weird_idx, \"credit_account\"] = rng.choice(\n",
        "    [\"9999-Miscellaneous\", \"8888-Suspense\"], size=len(weird_idx)\n",
        ")\n",
        "\n",
        "# c) Odd posting times (middle of the night)\n",
        "night_idx = anomaly_indices[30:]\n",
        "df.loc[night_idx, \"date_time\"] = df.loc[night_idx, \"date_time\"].dt.normalize() + pd.to_timedelta(\n",
        "    rng.integers(0, 5, size=len(night_idx)), unit=\"h\"\n",
        ")  # 00:00–04:00\n",
        "\n",
        "# Mark which ones we deliberately made anomalous (for our own evaluation, not for the model)\n",
        "df[\"is_synthetic_anomaly\"] = False\n",
        "df.loc[anomaly_indices, \"is_synthetic_anomaly\"] = True\n",
        "\n",
        "# 6) Quick peek\n",
        "df.head(), df[\"is_synthetic_anomaly\"].value_counts()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPRjYsS3yrZh",
        "outputId": "f5240a56-7abf-4901-b757-abc213133df0"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(  transaction_id                  date_time   amount currency  \\\n",
              " 0           tx_1 2025-06-05 07:17:58.684456  -908.64      EUR   \n",
              " 1           tx_2 2025-10-06 09:17:58.684456   872.15      GBP   \n",
              " 2           tx_3 2025-09-14 08:17:58.684456  1610.79      GBP   \n",
              " 3           tx_4 2025-08-06 03:17:58.684456  1651.69      EUR   \n",
              " 4           tx_5 2025-08-05 00:17:58.684456   839.21      EUR   \n",
              " \n",
              "            debit_account        credit_account  user_id             vendor  \\\n",
              " 0  6300-ProfessionalFees  2100-AccruedExpenses   user_4  Umbrella Services   \n",
              " 1            6100-Travel  2200-DeferredRevenue   user_5          Globex IT   \n",
              " 2                6200-IT             1000-Cash  user_15     Stark Supplies   \n",
              " 3          6500-Salaries               2000-AP   user_5     Stark Supplies   \n",
              " 4            6100-Travel  2200-DeferredRevenue   user_2     Stark Supplies   \n",
              " \n",
              "   transaction_type  is_synthetic_anomaly  \n",
              " 0          payment                 False  \n",
              " 1       adjustment                 False  \n",
              " 2          invoice                 False  \n",
              " 3       adjustment                 False  \n",
              " 4          invoice                 False  ,\n",
              " is_synthetic_anomaly\n",
              " False    4960\n",
              " True       40\n",
              " Name: count, dtype: int64)"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 5 — Save the Synthetic Dataset**\n",
        "\n",
        "This step writes the synthetic transaction data to a CSV file so it can be reloaded later in the notebook or shared externally.\n",
        "\n",
        "What this does:\n",
        "\n",
        "- Saves the entire df DataFrame (including synthetic anomalies and all transaction fields)\n",
        "to a file named transactions.csv in the Colab working directory.\n",
        "- Removes the index column for a clean CSV.\n",
        "- Prints a confirmation message with the number of rows saved.\n",
        "\n",
        "Why we do this:\n",
        "\n",
        "- Makes the dataset persistent for the rest of the notebook.\n",
        "- Allows us to download the file for inspection.\n",
        "- Ensures the next steps (feature engineering, anomaly detection) start from a clean load, simulating a real-world ingestion pipeline."
      ],
      "metadata": {
        "id": "l7KqRH0QLApS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(\"transactions.csv\", index=False)\n",
        "print(\"Saved synthetic data to transactions.csv with\", len(df), \"rows.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S76xDCw7zDjn",
        "outputId": "e86fa8e1-3581-4bee-fe79-e5d14e14a905"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved synthetic data to transactions.csv with 5000 rows.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 6 — Load the Transaction Dataset**\n",
        "\n",
        "This step loads the saved synthetic transaction data back into the notebook from the CSV file.\n",
        "\n",
        "What this does:\n",
        "\n",
        "- Reads transactions.csv into a pandas DataFrame called df.\n",
        "- Parses the date_time column as a proper datetime object so we can extract hours, weekdays, months, etc. later.\n",
        "- Ensures the rest of the pipeline starts from a clean, reproducible dataset—simulating a real ingestion step.\n",
        "\n",
        "Why this step matters:\n",
        "\n",
        "- Keeps the workflow consistent with how real production pipelines work (load → process → analyze).\n",
        "- Ensures that any accidental in-memory modifications from earlier cells don’t leak into the anomaly detection steps.\n",
        "- Makes it easy for others to run the notebook independently—everything is loaded from the saved CSV."
      ],
      "metadata": {
        "id": "z_U73P3I0Ls1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"transactions.csv\", parse_dates=[\"date_time\"])"
      ],
      "metadata": {
        "id": "FrEOZ0VV0LVQ"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 7 — Add Statistical Context (Per-Account & Per-Vendor Baselines)**\n",
        "\n",
        "This step enriches the dataset with statistical baselines that show what “normal” behaviour looks like for each account and vendor.\n",
        "These baselines provide ground truth context for the LLM so its explanations are factual rather than guessed.\n",
        "\n",
        "What this code does\n",
        "\n",
        "1. Create base_df\n",
        "   -  A copy of the dataset for feature engineering and modelling.\n",
        "2. Compute per-account statistics\n",
        "   - For each debit_account, we calculate:\n",
        "     - acct_avg_amount – average amount\n",
        "     - acct_median_amount – median amount\n",
        "     - acct_std_amount – standard deviation\n",
        "     - acct_tx_count – number of historical transactions\n",
        "   - These reflect typical behaviour per account.\n",
        "\n",
        "3. Compute per-vendor statistics\n",
        "   - Same logic as above, but grouped by vendor:\n",
        "     - vendor_avg_amount\n",
        "     - vendor_median_amount\n",
        "     - vendor_std_amount\n",
        "     - vendor_tx_count\n",
        "\n",
        "   - This shows whether a particular vendor is being used unusually.\n",
        "\n",
        "4. Merge the stats into the main dataset\n",
        "\n",
        "Each row now has its account/vendor baselines included.\n",
        "\n",
        "5. Calculate a Z-score vs account\n",
        "\n",
        "This shows how extreme the amount is compared to the account’s historical norm:\n",
        "\n",
        "```\n",
        "z = (amount - account average) / account standard deviation\n",
        "```\n",
        "\n",
        "  - High positive z → bigger than typical\n",
        "  - High negative z → unusually small or negative\n",
        "  - A great signal for the LLM explanations\n",
        "\n",
        "6. Clean up invalid Z-scores\n",
        "\n",
        "Prevents inf or NaN values when account std is 0.\n",
        "\n",
        "Why this step matters\n",
        "\n",
        "- IsolationForest learns structure, but it does not explain it.\n",
        "- The LLM explanations need concrete numbers to avoid hallucinations.\n",
        "- These statistical fields give the LLM real context like:\n",
        "\n",
        "> This amount is 3.8× higher than the typical average for account 6100-Travel.\n",
        "\n",
        "This creates explanations that are accurate, specific, and trustworthy."
      ],
      "metadata": {
        "id": "qLIuQNXDAENy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the same df you used for IsolationForest training (e.g. df_model or df)\n",
        "base_df = df.copy()\n",
        "\n",
        "# Make sure amount is numeric\n",
        "base_df[\"amount\"] = base_df[\"amount\"].astype(float)\n",
        "\n",
        "# 1) Stats by debit_account\n",
        "acct_stats = (\n",
        "    base_df.groupby(\"debit_account\")[\"amount\"]\n",
        "    .agg(\n",
        "        acct_avg_amount=\"mean\",\n",
        "        acct_median_amount=\"median\",\n",
        "        acct_std_amount=\"std\",\n",
        "        acct_tx_count=\"count\",\n",
        "    )\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "# 2) Stats by vendor\n",
        "vendor_stats = (\n",
        "    base_df.groupby(\"vendor\")[\"amount\"]\n",
        "    .agg(\n",
        "        vendor_avg_amount=\"mean\",\n",
        "        vendor_median_amount=\"median\",\n",
        "        vendor_std_amount=\"std\",\n",
        "        vendor_tx_count=\"count\",\n",
        "    )\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "# 3) Merge these stats back into base_df\n",
        "base_df = base_df.merge(acct_stats, on=\"debit_account\", how=\"left\")\n",
        "base_df = base_df.merge(vendor_stats, on=\"vendor\", how=\"left\")\n",
        "\n",
        "# If you want an amount z-score vs account\n",
        "base_df[\"acct_amount_zscore\"] = (\n",
        "    (base_df[\"amount\"] - base_df[\"acct_avg_amount\"]) / base_df[\"acct_std_amount\"]\n",
        ")\n",
        "\n",
        "# Replace inf/nan z-scores where std is 0 or missing\n",
        "base_df[\"acct_amount_zscore\"] = base_df[\"acct_amount_zscore\"].replace([np.inf, -np.inf], np.nan)\n"
      ],
      "metadata": {
        "id": "A2PqTwgj__iJ"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 8 — Create Model Features (Time Fields + Log Amount) and Build `df_model`**\n",
        "\n",
        "This step prepares the features that the IsolationForest model will learn from.\n",
        "We derive additional time-based fields, transform amounts, select categorical and numeric features, and build the modelling dataset.\n",
        "\n",
        "What this code does:\n",
        "\n",
        "1. Extract time-based behavioural features\n",
        "   - `hour` – posting time of day\n",
        "   - `dayofweek` – Monday=0 … Sunday=6\n",
        "   - `month` – captures seasonal or monthly patterns\n",
        "\n",
        "These help detect unusual posting behaviour (e.g., late-night or weekend transactions).\n",
        "\n",
        "2. Transform transaction amounts\n",
        "\n",
        "    `log_amount = log(1 + abs(amount))`\n",
        "\n",
        "   - Reduces impact of large outliers\n",
        "   - Makes the distribution more normal\n",
        "   - Helps IsolationForest detect subtle deviations\n",
        "\n",
        "3. Define numeric and categorical feature groups\n",
        "\n",
        "   - Numeric: log_amount, hour, dayofweek, month\n",
        "   - Categorical: accounts, user IDs, transaction type\n",
        "(These will later be one-hot encoded.)\n",
        "\n",
        "4. Build the model-ready dataset\n",
        "\n",
        "`df_model` includes:\n",
        "\n",
        "   - All modelling features\n",
        "   - The statistical context added in Step 7\n",
        "   - Only rows where all required modelling features are present\n",
        "\n",
        "This is what we feed into the preprocessing pipeline and IsolationForest model.\n",
        "\n",
        "What can be tweaked?\n",
        "\n",
        "- Add new time features (e.g., quarter, week number).\n",
        "- Add amount-based features (e.g., rolling averages).\n",
        "- Add more categorical fields (e.g., cost center, region).\n",
        "- Drop features if model behaves oddly.\n",
        "- Change feature_cols to test different modelling setups."
      ],
      "metadata": {
        "id": "298kNrUV0fiD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Time & amount features on base_df (NOT df) ---\n",
        "base_df[\"hour\"] = base_df[\"date_time\"].dt.hour\n",
        "base_df[\"dayofweek\"] = base_df[\"date_time\"].dt.dayofweek\n",
        "base_df[\"month\"] = base_df[\"date_time\"].dt.month\n",
        "base_df[\"log_amount\"] = np.log1p(base_df[\"amount\"].abs())\n",
        "base_df[\"raw_amount_abs\"] = base_df[\"amount\"].abs()\n",
        "\n",
        "# Features we feed into IsolationForest\n",
        "num_features = [\"log_amount\", \"raw_amount_abs\", \"hour\", \"dayofweek\", \"month\"]\n",
        "cat_features = [\"debit_account\", \"credit_account\", \"user_id\", \"transaction_type\"]\n",
        "\n",
        "feature_cols = num_features + cat_features\n",
        "\n",
        "# df_model now comes from base_df so it includes the stats columns too\n",
        "df_model = base_df.dropna(subset=feature_cols).copy()"
      ],
      "metadata": {
        "id": "KXJt9Uu_0i9j"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 9 — Build the ML Pipeline, Train IsolationForest, and Identify Anomalies**\n",
        "\n",
        "This step constructs a full preprocessing + modelling pipeline, trains the IsolationForest model, and computes anomaly scores for every transaction.\n",
        "\n",
        "What this does:\n",
        "\n",
        "- Preprocesses features\n",
        "  - Numeric fields pass through\n",
        "  - Categorical fields are one-hot encoded\n",
        "- Creates and trains an IsolationForest model\n",
        "  - Learns what “normal” transactions look like\n",
        "  - Flags roughly 1% as anomalies (contamination=0.01)\n",
        "- Generates anomaly scores\n",
        "  - More negative → more unusual\n",
        "  - `is_anomaly` = -1 marks suspicious rows\n",
        "- Builds `df_anomalies`, sorted from most to least anomalous.\n",
        "\n",
        "What you can tweak:\n",
        "\n",
        "- `contamination`=`0.01` → controls how sensitive you want the system to be\n",
        "   - 0.005 = stricter\n",
        "   - 0.02 = more anomalies flagged\n",
        "- `n_estimators` → more trees = more stable scoring\n",
        "- Add or remove features to see how model behaviour changes\n"
      ],
      "metadata": {
        "id": "V1hE-mDE0tyK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", \"passthrough\", num_features),\n",
        "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_features),\n",
        "    ]\n",
        ")\n",
        "\n",
        "iso_forest = IsolationForest(\n",
        "    n_estimators=200,\n",
        "    contamination=0.02,  # assume ~1% anomalies\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "pipe = Pipeline(steps=[\n",
        "    (\"prep\", preprocessor),\n",
        "    (\"model\", iso_forest),\n",
        "])\n",
        "\n",
        "pipe.fit(df_model[feature_cols])\n",
        "\n",
        "# Scores & predictions\n",
        "X_transformed = pipe[\"prep\"].transform(df_model[feature_cols])\n",
        "df_model[\"anomaly_score\"] = pipe[\"model\"].decision_function(X_transformed)\n",
        "df_model[\"is_anomaly\"] = pipe[\"model\"].predict(X_transformed)  # -1 anomaly, 1 normal\n",
        "\n",
        "df_anomalies = df_model[df_model[\"is_anomaly\"] == -1].copy()\n",
        "df_anomalies = df_anomalies.sort_values(\"anomaly_score\")  # more negative = more anomalous\n",
        "\n",
        "print(\"Anomalies found:\", (df_model[\"is_anomaly\"] == -1).sum())\n"
      ],
      "metadata": {
        "id": "ys3IMHPu0vE4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "610cb6da-bb3c-42af-b1c2-f3c189c84bb5"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Anomalies found: 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 10 — Select the Top Anomalies for LLM Explanation**\n",
        "\n",
        "This step chooses the most suspicious transactions to send to the LLM for interpretation.\n",
        "\n",
        "What this does:\n",
        "\n",
        "- Takes the first N rows from df_anomalies, which are already sorted by anomaly score.\n",
        "- These represent the most unusual transactions in the dataset.\n",
        "- `df_top` will be passed to the OpenAI model to generate human-friendly explanations.\n",
        "\n",
        "What you can tweak:\n",
        "\n",
        "- Change TOP_N to adjust how many anomalies you want explained:\n",
        "  - TOP_N = 5 → faster, good for testing\n",
        "  - TOP_N = 20 or 50 → richer analysis for stakeholders\n",
        "- Increase it after you're happy with the model behaviour.\n",
        "\n",
        "Be mindful that you'll burn through your OpenAI credits"
      ],
      "metadata": {
        "id": "xg_MINty00Cs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TOP_N = 10\n",
        "df_top = df_anomalies.head(TOP_N)"
      ],
      "metadata": {
        "id": "hj-esfkn01qS"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 11 — Define the LLM Prompt and Create a Function to Explain Anomalies**\n",
        "\n",
        "This step sets up the system prompt for the LLM and defines a reusable function that sends one anomalous transaction to OpenAI and returns a structured JSON explanation.\n",
        "\n",
        "What this does:\n",
        "\n",
        "1. Defines the system prompt\n",
        "\n",
        "   The SYSTEM_PROMPT tells the model exactly how to behave:\n",
        "\n",
        "    - It receives a single transaction and optional statistical context.\n",
        "    - It must return:\n",
        "      - severity\n",
        "      - category\n",
        "      - short explanation\n",
        "      - detailed explanation\n",
        "      - suggested action\n",
        "\n",
        "    - It must use the statistical values when available (e.g., averages, medians, z-scores).\n",
        "    - It must not invent numbers.\n",
        "    - It must reply with valid JSON only.\n",
        "\n",
        "This ensures clear, consistent, audit-friendly output.\n",
        "\n",
        "2. Builds a clean transaction payload\n",
        "\n",
        "   The function extracts relevant fields from each row:\n",
        "\n",
        "    - Transaction metadata (amount, accounts, date, vendor, etc.)\n",
        "    - Anomaly score\n",
        "    - Statistical baselines (avg, median, tx counts, z-scores)\n",
        "\n",
        "If any stat is missing, it is passed as null, and the model is instructed to handle that case gracefully.\n",
        "\n",
        "3. Sends the request to OpenAI\n",
        "   - Uses OpenAI’s structured output mode so the response is guaranteed to be JSON.\n",
        "   - Ensures the model behaves predictably across all anomalies.\n",
        "\n",
        "4. Parses the model output\n",
        "\n",
        "   - If valid JSON → return it.\n",
        "   - If anything goes wrong → return a fallback explanation so the pipeline never breaks.\n",
        "\n",
        "**Why this step matters**\n",
        "\n",
        "This function connects the ML anomaly score with human-readable insights:\n",
        "\n",
        "   - ML identifies what’s unusual.\n",
        "   - LLM explains why it’s unusual, using the statistical context you computed.\n",
        "\n",
        "This pairing is what makes the PoC feel intelligent and usable to finance teams.\n",
        "\n",
        "**What you can tweak:**\n",
        "\n",
        "   - Add/remove fields from the transaction payload.\n",
        "   - Modify severity logic instructions.\n",
        "   - Add more categories (e.g., vendor anomaly, duplicate risk).\n",
        "   - Adjust the tone of the explanations.\n",
        "   - Switch to a different OpenAI model by changing model=MODEL_NAME."
      ],
      "metadata": {
        "id": "2CcXknkg057A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "SYSTEM_PROMPT = \"\"\"\n",
        "You are an assistant for a subledger accounting platform.\n",
        "\n",
        "You receive:\n",
        "- A single transaction with fields like amount, accounts, user, date.\n",
        "- An anomaly score from an IsolationForest model (more negative = more anomalous).\n",
        "- Optional statistical context about typical amounts for this account and vendor.\n",
        "\n",
        "The statistical fields (when present) are:\n",
        "- acct_avg_amount: average amount for this debit_account.\n",
        "- acct_median_amount: median amount for this debit_account.\n",
        "- acct_tx_count: number of historical transactions for this debit_account.\n",
        "- vendor_avg_amount: average amount for this vendor.\n",
        "- vendor_median_amount: median amount for this vendor.\n",
        "- vendor_tx_count: number of historical transactions for this vendor.\n",
        "- acct_amount_zscore: (amount - acct_avg_amount) / acct_std_amount.\n",
        "\n",
        "Your job:\n",
        "1. Decide severity: \"low\", \"medium\", or \"high\".\n",
        "2. Choose category from: [\"amount_anomaly\", \"timing_anomaly\", \"account_combo_anomaly\", \"user_behavior_anomaly\", \"other\"].\n",
        "3. Provide a short, one-sentence explanation appropriate for a dashboard.\n",
        "4. Provide a detailed explanation (2–5 sentences) with specific numbers.\n",
        "5. Suggest a concrete next action for a finance user.\n",
        "\n",
        "Rules:\n",
        "- Use the statistical context when available. For example:\n",
        "  \"This amount is about 4.5x the average for this account (X vs typical Y).\"\n",
        "- Do NOT claim that you know \"typical\" amounts unless the statistical fields are present.\n",
        "- If the statistical fields are missing or null, speak more generally:\n",
        "  e.g. \"The anomaly score indicates this transaction is unusual, but the exact deviation from normal is not provided.\"\n",
        "- Do not invent numbers.\n",
        "- Keep explanations factual and concise.\n",
        "- Return valid JSON ONLY with keys:\n",
        "  [\"severity\", \"category\", \"short_explanation\", \"detailed_explanation\", \"suggested_action\"].\n",
        "\"\"\"\n",
        "\n",
        "def call_openai_for_anomaly(row, client, model=MODEL_NAME):\n",
        "\n",
        "    \"\"\"Call OpenAI to explain a single anomalous transaction.\"\"\"\n",
        "\n",
        "    # Safe date handling\n",
        "    date_time = row[\"date_time\"]\n",
        "    if hasattr(date_time, \"isoformat\"):\n",
        "        date_time_str = date_time.isoformat()\n",
        "    else:\n",
        "        date_time_str = str(date_time)\n",
        "\n",
        "    tx = {\n",
        "        \"transaction_id\": str(row[\"transaction_id\"]),\n",
        "        \"amount\": float(row[\"amount\"]),\n",
        "        \"currency\": row.get(\"currency\"),\n",
        "        \"date_time\": row[\"date_time\"].isoformat(),\n",
        "        \"debit_account\": row.get(\"debit_account\"),\n",
        "        \"credit_account\": row.get(\"credit_account\"),\n",
        "        \"user_id\": row.get(\"user_id\"),\n",
        "        \"vendor\": row.get(\"vendor\"),\n",
        "        \"transaction_type\": row.get(\"transaction_type\"),\n",
        "        \"anomaly_score\": float(row[\"anomaly_score\"]),\n",
        "        # statistical context (may be NaN → will serialise as null)\n",
        "        \"acct_avg_amount\": row.get(\"acct_avg_amount\"),\n",
        "        \"acct_median_amount\": row.get(\"acct_median_amount\"),\n",
        "        \"acct_tx_count\": row.get(\"acct_tx_count\"),\n",
        "        \"vendor_avg_amount\": row.get(\"vendor_avg_amount\"),\n",
        "        \"vendor_median_amount\": row.get(\"vendor_median_amount\"),\n",
        "        \"vendor_tx_count\": row.get(\"vendor_tx_count\"),\n",
        "        \"acct_amount_zscore\": row.get(\"acct_amount_zscore\"),\n",
        "    }\n",
        "\n",
        "    user_prompt = f\"\"\"\n",
        "\n",
        "    Here is an anomalous transaction from an accounting subledger system, including optional statistical context:\n",
        "\n",
        "    ```json\n",
        "    {json.dumps(tx, indent=2)}\n",
        "    Follow the rules and output the JSON response only.\n",
        "    ```\n",
        "    \"\"\"\n",
        "\n",
        "    # Use JSON response format so we reliably get JSON back\n",
        "    completion = client.chat.completions.create(\n",
        "        model=model,\n",
        "        response_format={\"type\": \"json_object\"},\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "            {\"role\": \"user\", \"content\": user_prompt},\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    text = completion.choices[0].message.content\n",
        "\n",
        "    try:\n",
        "        return json.loads(text)\n",
        "    except json.JSONDecodeError:\n",
        "        # Fallback if something weird happens\n",
        "        return {\n",
        "            \"severity\": \"unknown\",\n",
        "            \"category\": \"other\",\n",
        "            \"short_explanation\": \"Model returned invalid JSON.\",\n",
        "            \"detailed_explanation\": text,\n",
        "            \"suggested_action\": \"Review this transaction manually.\",\n",
        "        }"
      ],
      "metadata": {
        "id": "KhZcxKhv07zP"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 12 — Generate LLM Explanations and Build the Final Enriched Output**\n",
        "\n",
        "This step loops through the top anomalous transactions, sends each one to the LLM for interpretation, and combines everything into a final, enriched dataset.\n",
        "\n",
        "What this does:\n",
        "\n",
        "1. Calls the LLM for each anomaly\n",
        "   - Iterates over the most suspicious transactions (`df_top`).\n",
        "   - For each one, calls the OpenAI function from Step 11.\n",
        "   - Stores each JSON explanation in a list.\n",
        "\n",
        "2. Combines original data + LLM output\n",
        "\n",
        "   Merges:\n",
        "   - the original anomaly data (`df_top`)\n",
        "   - the LLM-generated fields (severity, category, explanations, etc.)\n",
        "\n",
        "   into one unified DataFrame.\n",
        "\n",
        "3. Displays the enriched dashboard-ready output\n",
        "\n",
        "   The preview includes:\n",
        "\n",
        "    - key transaction fields\n",
        "    - statistical context\n",
        "    - ML anomaly score\n",
        "    - LLM severity + category\n",
        "    - LLM short explanation\n",
        "    - Suggested next action\n",
        "\n",
        "   This is the final PoC output you would surface to a finance user or integrate into a UI.\n",
        "\n",
        "What you can tweak:\n",
        "   - Add more fields to the preview (e.g., z-scores, vendor stats).\n",
        "   - Increase or decrease the number of rows displayed.\n",
        "   - Capture model reasoning for auditing (LLM “thoughts” not included by default).\n",
        "   - Add IDs or correlation IDs for tracking."
      ],
      "metadata": {
        "id": "9oUmcbHaQBnx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "explanations = []\n",
        "for _, row in df_top.iterrows():\n",
        "    explanations.append(call_openai_for_anomaly(row, client))\n",
        "\n",
        "df_enriched = pd.concat(\n",
        "    [df_top.reset_index(drop=True), pd.DataFrame(explanations)],\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "df_enriched[[\n",
        "    \"transaction_id\", \"date_time\", \"amount\", \"debit_account\", \"credit_account\", \"vendor\",\n",
        "    \"acct_avg_amount\", \"vendor_avg_amount\", \"anomaly_score\",\n",
        "    \"severity\", \"category\", \"short_explanation\", \"detailed_explanation\", \"suggested_action\"\n",
        "]].head(10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        },
        "id": "M9_sTg1z1Uli",
        "outputId": "652b19f3-5efe-4bbf-907a-b1f354f000a1"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  transaction_id                  date_time     amount          debit_account  \\\n",
              "0         tx_465 2025-05-20 23:17:58.684456  177103.00              6400-Rent   \n",
              "1        tx_1140 2025-08-24 22:17:58.684456   -3029.55            6100-Travel   \n",
              "2        tx_4856 2025-09-20 23:17:58.684456     812.88          6500-Salaries   \n",
              "3         tx_623 2025-06-17 22:17:58.684456    3097.52  6300-ProfessionalFees   \n",
              "4        tx_4291 2025-06-01 02:17:58.684456    2677.30                6200-IT   \n",
              "5        tx_3562 2025-06-18 23:17:58.684456     538.69                6200-IT   \n",
              "6        tx_1216 2025-06-08 23:17:58.684456    1341.07         6000-Marketing   \n",
              "7        tx_3919 2025-05-30 03:17:58.684456  150382.61                6200-IT   \n",
              "8         tx_362 2025-10-26 22:17:58.684456    -420.83                6200-IT   \n",
              "9         tx_777 2025-08-03 23:17:58.684456    -477.83            6100-Travel   \n",
              "\n",
              "         credit_account              vendor  acct_avg_amount  \\\n",
              "0               2000-AP        Wayne Travel      1637.711580   \n",
              "1  2200-DeferredRevenue         ACME Events       957.658634   \n",
              "2  2200-DeferredRevenue  Initech Consulting       804.215667   \n",
              "3             1000-Cash           Globex IT      1237.957393   \n",
              "4  2100-AccruedExpenses  Initech Consulting      1030.313540   \n",
              "5               2000-AP   Umbrella Services      1030.313540   \n",
              "6               2000-AP         ACME Events      1101.998086   \n",
              "7             1000-Cash        Wayne Travel      1030.313540   \n",
              "8               2000-AP           Globex IT      1030.313540   \n",
              "9  2100-AccruedExpenses  Initech Consulting       957.658634   \n",
              "\n",
              "   vendor_avg_amount  anomaly_score severity        category  \\\n",
              "0        1790.871535      -0.035896     high  amount_anomaly   \n",
              "1         809.589852      -0.024372      low  amount_anomaly   \n",
              "2        1367.780524      -0.020313      low  amount_anomaly   \n",
              "3         805.650876      -0.018653      low  amount_anomaly   \n",
              "4        1367.780524      -0.017552      low  amount_anomaly   \n",
              "5         829.720766      -0.017024      low  amount_anomaly   \n",
              "6         809.589852      -0.016333      low  amount_anomaly   \n",
              "7        1790.871535      -0.015366     high  amount_anomaly   \n",
              "8         805.650876      -0.014981      low  amount_anomaly   \n",
              "9        1367.780524      -0.014658      low  amount_anomaly   \n",
              "\n",
              "                                   short_explanation  \\\n",
              "0  This rent payment amount is highly anomalous c...   \n",
              "1  The transaction amount is somewhat lower than ...   \n",
              "2  This transaction amount closely aligns with ty...   \n",
              "3  This transaction amount is somewhat higher tha...   \n",
              "4  Transaction amount is moderately higher than t...   \n",
              "5  Transaction amount is slightly below typical a...   \n",
              "6  Transaction amount is slightly above average b...   \n",
              "7  This transaction amount is extremely high comp...   \n",
              "8  This transaction amount is slightly lower than...   \n",
              "9  The transaction amount is negative and slightl...   \n",
              "\n",
              "                                detailed_explanation  \\\n",
              "0  The transaction amount of $177,103 is about 10...   \n",
              "1  The transaction amount is -3029.55 GBP, which ...   \n",
              "2  The transaction amount of 812.88 EUR is very c...   \n",
              "3  The transaction amount of 3097.52 EUR is about...   \n",
              "4  This transaction amount of 2677.3 EUR is about...   \n",
              "5  This transaction amount of 538.69 EUR is below...   \n",
              "6  The transaction amount of 1341.07 EUR is about...   \n",
              "7  The transaction amount of EUR 150,382.61 is ab...   \n",
              "8  The transaction amount is -420.83 EUR, which i...   \n",
              "9  This transaction has an amount of -477.83 EUR,...   \n",
              "\n",
              "                                    suggested_action  \n",
              "0  Review this transaction in detail to verify it...  \n",
              "1  Review the nature of this adjustment transacti...  \n",
              "2  Review transaction details briefly to confirm ...  \n",
              "3  Review the transaction details for justificati...  \n",
              "4  Review the transaction details to confirm appr...  \n",
              "5  Review this transaction to confirm the lower a...  \n",
              "6  Review the transaction details to confirm legi...  \n",
              "7  Review the transaction details and supporting ...  \n",
              "8  Review the transaction details to confirm the ...  \n",
              "9  Review the transaction details to confirm that...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-55c9f589-4a2d-42a7-ae15-64b2d0ca5247\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>transaction_id</th>\n",
              "      <th>date_time</th>\n",
              "      <th>amount</th>\n",
              "      <th>debit_account</th>\n",
              "      <th>credit_account</th>\n",
              "      <th>vendor</th>\n",
              "      <th>acct_avg_amount</th>\n",
              "      <th>vendor_avg_amount</th>\n",
              "      <th>anomaly_score</th>\n",
              "      <th>severity</th>\n",
              "      <th>category</th>\n",
              "      <th>short_explanation</th>\n",
              "      <th>detailed_explanation</th>\n",
              "      <th>suggested_action</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>tx_465</td>\n",
              "      <td>2025-05-20 23:17:58.684456</td>\n",
              "      <td>177103.00</td>\n",
              "      <td>6400-Rent</td>\n",
              "      <td>2000-AP</td>\n",
              "      <td>Wayne Travel</td>\n",
              "      <td>1637.711580</td>\n",
              "      <td>1790.871535</td>\n",
              "      <td>-0.035896</td>\n",
              "      <td>high</td>\n",
              "      <td>amount_anomaly</td>\n",
              "      <td>This rent payment amount is highly anomalous c...</td>\n",
              "      <td>The transaction amount of $177,103 is about 10...</td>\n",
              "      <td>Review this transaction in detail to verify it...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>tx_1140</td>\n",
              "      <td>2025-08-24 22:17:58.684456</td>\n",
              "      <td>-3029.55</td>\n",
              "      <td>6100-Travel</td>\n",
              "      <td>2200-DeferredRevenue</td>\n",
              "      <td>ACME Events</td>\n",
              "      <td>957.658634</td>\n",
              "      <td>809.589852</td>\n",
              "      <td>-0.024372</td>\n",
              "      <td>low</td>\n",
              "      <td>amount_anomaly</td>\n",
              "      <td>The transaction amount is somewhat lower than ...</td>\n",
              "      <td>The transaction amount is -3029.55 GBP, which ...</td>\n",
              "      <td>Review the nature of this adjustment transacti...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>tx_4856</td>\n",
              "      <td>2025-09-20 23:17:58.684456</td>\n",
              "      <td>812.88</td>\n",
              "      <td>6500-Salaries</td>\n",
              "      <td>2200-DeferredRevenue</td>\n",
              "      <td>Initech Consulting</td>\n",
              "      <td>804.215667</td>\n",
              "      <td>1367.780524</td>\n",
              "      <td>-0.020313</td>\n",
              "      <td>low</td>\n",
              "      <td>amount_anomaly</td>\n",
              "      <td>This transaction amount closely aligns with ty...</td>\n",
              "      <td>The transaction amount of 812.88 EUR is very c...</td>\n",
              "      <td>Review transaction details briefly to confirm ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>tx_623</td>\n",
              "      <td>2025-06-17 22:17:58.684456</td>\n",
              "      <td>3097.52</td>\n",
              "      <td>6300-ProfessionalFees</td>\n",
              "      <td>1000-Cash</td>\n",
              "      <td>Globex IT</td>\n",
              "      <td>1237.957393</td>\n",
              "      <td>805.650876</td>\n",
              "      <td>-0.018653</td>\n",
              "      <td>low</td>\n",
              "      <td>amount_anomaly</td>\n",
              "      <td>This transaction amount is somewhat higher tha...</td>\n",
              "      <td>The transaction amount of 3097.52 EUR is about...</td>\n",
              "      <td>Review the transaction details for justificati...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>tx_4291</td>\n",
              "      <td>2025-06-01 02:17:58.684456</td>\n",
              "      <td>2677.30</td>\n",
              "      <td>6200-IT</td>\n",
              "      <td>2100-AccruedExpenses</td>\n",
              "      <td>Initech Consulting</td>\n",
              "      <td>1030.313540</td>\n",
              "      <td>1367.780524</td>\n",
              "      <td>-0.017552</td>\n",
              "      <td>low</td>\n",
              "      <td>amount_anomaly</td>\n",
              "      <td>Transaction amount is moderately higher than t...</td>\n",
              "      <td>This transaction amount of 2677.3 EUR is about...</td>\n",
              "      <td>Review the transaction details to confirm appr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>tx_3562</td>\n",
              "      <td>2025-06-18 23:17:58.684456</td>\n",
              "      <td>538.69</td>\n",
              "      <td>6200-IT</td>\n",
              "      <td>2000-AP</td>\n",
              "      <td>Umbrella Services</td>\n",
              "      <td>1030.313540</td>\n",
              "      <td>829.720766</td>\n",
              "      <td>-0.017024</td>\n",
              "      <td>low</td>\n",
              "      <td>amount_anomaly</td>\n",
              "      <td>Transaction amount is slightly below typical a...</td>\n",
              "      <td>This transaction amount of 538.69 EUR is below...</td>\n",
              "      <td>Review this transaction to confirm the lower a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>tx_1216</td>\n",
              "      <td>2025-06-08 23:17:58.684456</td>\n",
              "      <td>1341.07</td>\n",
              "      <td>6000-Marketing</td>\n",
              "      <td>2000-AP</td>\n",
              "      <td>ACME Events</td>\n",
              "      <td>1101.998086</td>\n",
              "      <td>809.589852</td>\n",
              "      <td>-0.016333</td>\n",
              "      <td>low</td>\n",
              "      <td>amount_anomaly</td>\n",
              "      <td>Transaction amount is slightly above average b...</td>\n",
              "      <td>The transaction amount of 1341.07 EUR is about...</td>\n",
              "      <td>Review the transaction details to confirm legi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>tx_3919</td>\n",
              "      <td>2025-05-30 03:17:58.684456</td>\n",
              "      <td>150382.61</td>\n",
              "      <td>6200-IT</td>\n",
              "      <td>1000-Cash</td>\n",
              "      <td>Wayne Travel</td>\n",
              "      <td>1030.313540</td>\n",
              "      <td>1790.871535</td>\n",
              "      <td>-0.015366</td>\n",
              "      <td>high</td>\n",
              "      <td>amount_anomaly</td>\n",
              "      <td>This transaction amount is extremely high comp...</td>\n",
              "      <td>The transaction amount of EUR 150,382.61 is ab...</td>\n",
              "      <td>Review the transaction details and supporting ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>tx_362</td>\n",
              "      <td>2025-10-26 22:17:58.684456</td>\n",
              "      <td>-420.83</td>\n",
              "      <td>6200-IT</td>\n",
              "      <td>2000-AP</td>\n",
              "      <td>Globex IT</td>\n",
              "      <td>1030.313540</td>\n",
              "      <td>805.650876</td>\n",
              "      <td>-0.014981</td>\n",
              "      <td>low</td>\n",
              "      <td>amount_anomaly</td>\n",
              "      <td>This transaction amount is slightly lower than...</td>\n",
              "      <td>The transaction amount is -420.83 EUR, which i...</td>\n",
              "      <td>Review the transaction details to confirm the ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>tx_777</td>\n",
              "      <td>2025-08-03 23:17:58.684456</td>\n",
              "      <td>-477.83</td>\n",
              "      <td>6100-Travel</td>\n",
              "      <td>2100-AccruedExpenses</td>\n",
              "      <td>Initech Consulting</td>\n",
              "      <td>957.658634</td>\n",
              "      <td>1367.780524</td>\n",
              "      <td>-0.014658</td>\n",
              "      <td>low</td>\n",
              "      <td>amount_anomaly</td>\n",
              "      <td>The transaction amount is negative and slightl...</td>\n",
              "      <td>This transaction has an amount of -477.83 EUR,...</td>\n",
              "      <td>Review the transaction details to confirm that...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-55c9f589-4a2d-42a7-ae15-64b2d0ca5247')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-55c9f589-4a2d-42a7-ae15-64b2d0ca5247 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-55c9f589-4a2d-42a7-ae15-64b2d0ca5247');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-a5912cd6-b119-4d4e-941a-971da3a27cbc\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a5912cd6-b119-4d4e-941a-971da3a27cbc')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-a5912cd6-b119-4d4e-941a-971da3a27cbc button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"]]\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"transaction_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"tx_362\",\n          \"tx_1140\",\n          \"tx_3562\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"date_time\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2025-05-20 23:17:58.684456\",\n        \"max\": \"2025-10-26 22:17:58.684456\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"2025-10-26 22:17:58.684456\",\n          \"2025-08-24 22:17:58.684456\",\n          \"2025-06-18 23:17:58.684456\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"amount\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 69109.77324454048,\n        \"min\": -3029.55,\n        \"max\": 177103.0,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          -420.83,\n          -3029.55,\n          538.69\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"debit_account\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"6400-Rent\",\n          \"6100-Travel\",\n          \"6000-Marketing\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"credit_account\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"2200-DeferredRevenue\",\n          \"2100-AccruedExpenses\",\n          \"2000-AP\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"vendor\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"ACME Events\",\n          \"Umbrella Services\",\n          \"Initech Consulting\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"acct_avg_amount\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 224.17983827527925,\n        \"min\": 804.2156674757281,\n        \"max\": 1637.7115795868772,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          1637.7115795868772,\n          957.6586342042755,\n          1101.9980863477247\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"vendor_avg_amount\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 412.2015039280178,\n        \"min\": 805.6508757396449,\n        \"max\": 1790.8715349887132,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          809.5898520345253,\n          829.7207663316583,\n          1367.7805244755243\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"anomaly_score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.006453681162441031,\n        \"min\": -0.035895896155945406,\n        \"max\": -0.014658204385314566,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          -0.014980896424069134,\n          -0.024371832579375197,\n          -0.017023544058455686\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"severity\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"low\",\n          \"high\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"category\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"amount_anomaly\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"short_explanation\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"This transaction amount is slightly lower than usual for this account and vendor but close to typical values.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"detailed_explanation\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"The transaction amount is -420.83 EUR, which is less than half the average amount for this debit account (1030.31 EUR) and the vendor (805.65 EUR). However, the z-score of -0.24 indicates the amount is within the expected range of variation and not greatly unusual. The anomaly score is only slightly negative, suggesting a low severity anomaly.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"suggested_action\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"Review the transaction details to confirm the adjustment is correct given its lower amount compared to typical transactions for this account and vendor.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 13 — Export the Dataset to CSV**\n",
        "\n",
        "This step saves the full dataset dataset—including ML scores, statistical context, and LLM explanations—to a CSV file for download or further analysis.\n",
        "\n",
        "What this does:\n",
        "\n",
        "- Creates a copy of the dataframe model and adds additional columns that are expected from the LLM.\n",
        "- Iterates through the anomaly dataset and updates the relevant fields where a match can be found.\n",
        "- Produces a final deliverable you can:\n",
        "  - download from Colab\n",
        "\n",
        "This completes the end-to-end PoC pipeline."
      ],
      "metadata": {
        "id": "PmiSlqQWQqCS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Start with a full copy of the dataset used by the model\n",
        "df_full_export = df_model.copy()\n",
        "\n",
        "# Add empty columns for LLM outputs\n",
        "for col in [\"severity\", \"category\", \"short_explanation\", \"detailed_explanation\", \"suggested_action\"]:\n",
        "    df_full_export[col] = None\n",
        "\n",
        "# Insert LLM explanations only for the anomalies we explained (df_top)\n",
        "for idx, row in df_enriched.iterrows():   # df_enriched matches df_top row order\n",
        "    original_idx = df_top.index[idx]      # get the original row index in df_model\n",
        "    for col in [\"severity\", \"category\", \"short_explanation\", \"detailed_explanation\", \"suggested_action\"]:\n",
        "        df_full_export.loc[original_idx, col] = row[col]\n",
        "\n",
        "# Save the complete dataset\n",
        "df_full_export.to_csv(\"full_dataset_with_selected_llm_explanations.csv\", index=False)\n",
        "\n",
        "print(\"Full dataset saved with LLM explanations only for selected anomalies.\")\n",
        "# df_enriched.to_csv(\"enriched_anomalies_openai.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "Q64B7JDV-TY5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffe3500c-e8b6-4abc-8556-93f2a46f33f5"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full dataset saved with LLM explanations only for selected anomalies.\n"
          ]
        }
      ]
    }
  ]
}